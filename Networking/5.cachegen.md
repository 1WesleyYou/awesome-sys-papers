# Summary of "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving"

Yuchen You (yuchenxr)

## Problem and Motivation

__Problem__:

- Current LLMs' input often require a long context containing thousands of tokens or even more, but current processing methods are inefficient since they need ot load the whole context and then can they generate answers.
- KV cache of a reused context may not always be in local GPU memory when the next input comes, so these resources needed to be fetched from another machine first, this introduce another source of latency to the model.

__Motivation__:

- Transmitting the reused context among training machines is duplicate and inefficient, cutting down the amount of the reused context transmission is promising to reduce the TTFT (time to first token) and thus improve the performance of LLM serving.
- CacheGen focues on the cutting down the netowrk delay for sending the KV cache to achieve low-latency LLM serving.

## Hypothesis

- KV caches, if needed to transmit among different machines, will probably transmit on a link with low bandwidth.
- Caching the intermediate results for long is promising to reduce TTFT and thus improve performance of LLM.
- CacheGen design could reduce the _transmission-time_ size of KV cache by encoding it into compact bitstreams to reduce the network delay of sending it.
- The KV Cache shrunk by the recent works can still be encoded to further reduce the KV cache size and the network delay of sending KV caches.

## Solution Overview

- CacheGen encodes a precomputed KV cache into more compact bitstream representations, rather than keeping the tensor shapes of the KV cache.
  - This could save bandwidth and delays when sending a KV cache.
- CacheGen streams the encoded bitstreams of a KV cache in a way that adapts to changes in network conditions.
  - Splits a long context into chunks and encodes the KV of each chunk separately at variaous compression levels.
  - When sending contexts' KV cache, CacheGen fetches the chunks one by one and adapts the per-chunk compression level to maintain high generation quality (like the DASH for video streaming), and that could keep the network delay within a SLO
- Decoding: receiver decode the KV cache bitstreams back to the original KV cache tensors.

## Limitations and Possible Improvements

- Tests are conducted only on 4 datasets and 3 models, not general enough, where the models are merely Llama and Mistral with no more than 70B but not those SOTA models with more parameters.
- Maybe could be tested on more kinds of models like multi-modal models.
