# Review Summary: NetLLM: Adapting Large Language Models for Networking

Yuchen You (yuchenxr)

## Problem and Motivation

- **Problem addressed**: The shortcoming of existing rule based approaches as well as the DL(learning)-based approaches in networking, whose ability of solving networking problems are limited to seen(training) data but behaves poorly on unseen data/problems.
- **Why important**: Current design philosophy requires a lot of engineering overhead with poor generalization on unseen data; Networking tasks need more robust, generalizable models to help solve all kinds of networking problems, yet existing solutions are not good enough.
- **Key idea (Motivation)**: Adapt LLMs to networking tasks, which could build a good agent for all networking related tasks and problems.
- **Insight behind these ideas**: LLMs can generate solution plans for solving many kinds of problems which could generalize to unseen environments, there are many real-world examples in other engineering fields.
- **Goal**: Use LLMs for better networking adaptation, aiming for stronger generalization and reduced human engineering cost.

## Hypothesis

- Domain-adapted LLMs will improve networking performance and generalization in solving related problems.
- Multimodal input encoder will help with many input method types in networking for more general cases and solutions.

## Solution Overview

- **Approach (Element)**:
  - Multimodal encoder for networking data, that could take in various types of data like video/audio, so we could find the best way of demonstrating networking problems.
  - Networking head for efficient output, so that the generated answers are limited to the desired output space.
  - DD-LRNA (Data-Driven Low Rank Networking Adaptation) to fine-tune LLMs efficiently.
- **Pipeline**: the main workflow is demonstrated in Figure 5 in the paper.
- **Experiments**:
  - Model: mainly Llama2-7B adapted with NetLLM.
  - Tasks: VP, ABR, CJS.
  - Baselines: TRACK for VP, GENET for ABR, and Decima for CJS.
  - Metrics: MAE (VP), QoE (ABR), JCT (CJS).
  - Hardware: Linux, Intel Xeon CPUs, NVIDIA A100 GPUs.
- **Results**: NetLLM consistently outperforms baselines in the tested networking tasks like VP, ABR, and CJS, demonstrating better performance and generalization on unseen data.
- **Discussions**: it contains comparison experiments in many aspects, like different LLM parameter size (should >= 1B), different LLMs (found Llama is the best), etc.

## Limitations and Possible Improvements

- **Limitations**:
  - Test on only 3 networking tasks (VP, ABR, CJS), not general enough, only works well on these tasks.
  - Multimodal input limited mainly to video/audio, the input forms could be more diverse.
  - Real-time latency and compute cost not addressed.
- **Missing aspects**: No robustness/adversarial evaluation, no inference latency or resource cost analysis, and limited real-world deployment validation.
- **Possible improvements**:
  - Conduct robustness and adversarial testing to ensure a practical safe (secure) model.
  - Evaluate real-time performance and resource efficiency.

## Conclusion (Not from the template)

- **Conclusion**: NetLLM demonstrates that LLMs can serve as foundation models for networking, reducing handcrafted (human-involved) costs and improving generalization.
